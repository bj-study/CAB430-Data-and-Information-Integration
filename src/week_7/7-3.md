# Performance Evaluation

## Evaluation
Performance evaluation is important in order to obtain an understanding of the
effectiveness and efficiency of various data mining algorithms.

There are generally two ways we evaluate performance:
1. **User-based**: People are recruited and the feedback they provide based on the data
mining models results are recorded.
    - This option can be difficult and expensive due to the large cohorts of participants generally needed
    - This option may also provide biased and non-representative feedback
2. **Data-based**: Historical data is used to measure the performance of data mining
algorithms. This is the more widely accepted and popular approach to performance evaluation.
    - Once a dataset has been collected, it can be used as a standardised benchmark
    to compare other various models.

## N-Fold Cross-validation
In this method, the dataset is divided into \\(n\\) equal sets. One of the sets
is used for testing the model, while the remaining sets are used for training the
model.

This process is repeated \\(n\\) times by using each of the \\(n\\) sets as the
testing set. The average results over the \\(n\\) different testing sets are then
used to measure the data mining models performance.

This method can closely estimate the result as all the data is used for both training
and testing.

## Accuracy Metrics
There are a few common evaluation metrics:
- Effectiveness: The accuracy
- Efficiency: The speed and memory usage
- Cost: The money, workload, etc...

### Numerical Metrics
There are also numerical value predictions via Mean Square Error (MSE) and Root Mean
Error (RMSE). 

- \\(X\\) is a set of testing cases to be predicted
- \\(|X|\\) is the number of cases in \\(X\\)
- \\(V_x\\) is the true value of case \\(x\\), and \\(P_x\\) is a predicted value for \\(x\\)

\\[
    MSE = \frac{\sum_{x} \in x(V_x - P_x)^2}{|X|}
\\]

\\[
    RMSE = \sqrt{\frac{\sum_{x} \in x(V_x - P_x)^2}{|X|}}
\\]

### Classification / Predictive Model Metrics
A confusion matrix can be used to evaluate classification or predictive models.
For an \\(N\\)-class prediction model, the matrix is an \\(N \times N\\) table.

For example, imaging two classes, a positive class and negative class:
- TP: true positive, number of positive cases which are predicted as positive
- FP: false positive, number of false cases which are predicted as positive
- TN: true negative, number of false cases which are predicted as false
- FN: false negative, number of positive cases which are predicted as negative

Evaluation:
- Pass: TP + TN
- Fail: FT + FN

For a 2-class prediction, some measures can be derived:
- Precision = \\(\frac{TP}{TP + FP}\\)
- Recall = \\(\frac{TP}{TP + FN}\\)
- F-score = \\(2 * \frac{Precision * Recall}{Precision + Recall}\\)
- Accuracy = \\(\frac{TP + TN}{TP + TN + FP + FN}\\)

## Evaluation in Analysis Services - Accuracy
Using stored procedures, we can generate different evaluation results via provided functions.

These different functions return a table that follows the general schema:

| Column Name | Description |
|-------------|-------------|
| ModelName | The name of the model that was tested |
| AttributeName | The name of the predictable column |
| AttributeState | A target value in the predictable column |
| PartitionIndex | Denotes the partition to which the results applies |
| PartitionCases | An integer that indicates the number of rows in the case set, based on the <data-ste> parameter |
| Test | The type of test that was performed |
| Measure | The name of the measure returned by the test. Measures for each model depend on the model type, and the type of the predictable value |
| Value | The value for the specified measure |

### SystemGetAccuracyResults
```
SystemGetAccuracyResults(
    <mining-structure> 
    [,<mining-model-list>]
    [,<data-set>]
    [,<target-attribute>]
    [,<target-state>]
    [,<target-threshold>]
)
```

For example:
```
CALL SystemGetAccuracyResults ([people3], [PredictGenderNested-Trees], 3, 'Gender', NULL)
```

### SystemGetCrossValidationResults
```
SystemGetAccuracyResults(
    <mining-structure> 
    [,<mining-model-list>]
    [,<fold-count>]
    [,<max-cases>]
    [,<target-attribute>]
    [,<target-state>]
    [,<target-threshold>]
)
```

For example:
```
CALL SystemGetCrossValidationResults ([people3], [PredictGenderNested-Trees], 2, 0, 'Gender', NULL)
```

## Evaluation in Analysis Services - Measures

- Classification: The confusion matrix
    - Measures: If the predictable attribute value is specified:
        - True positive
        - True negative
        - False positive
        - False negative
    - Otherwise: Pass/Fail - The number of cases whose predicted attribute matches/not matches the target state
- Likelihood:
    - Measures: List measures the dependency among attributes

